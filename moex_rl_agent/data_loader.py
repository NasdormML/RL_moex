import time
import pickle
import hashlib
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from . import features as feat

# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===
MOEX_BASE = "https://iss.moex.com/iss"
CACHE_DIR = Path(__file__).parent / "data" / "cache"
CACHE_DIR.mkdir(parents=True, exist_ok=True)

def create_session() -> requests.Session:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Å—Å–∏–∏ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º"""
    session = requests.Session()
    retry = Retry(
        total=5,
        backoff_factor=0.3,
        status_forcelist=[500, 502, 503, 504],
        allowed_methods=["GET"]
    )
    adapter = HTTPAdapter(max_retries=retry, pool_maxsize=10)
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    return session


def get_cache_path(ticker: str, board: str, start: datetime, end: datetime) -> Path:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—É—Ç–∏ –∫ –∫—ç—à-—Ñ–∞–π–ª—É –¥–ª—è —Ç–∏–∫–µ—Ä–∞"""
    key = f"{ticker}_{board}_{start.strftime('%Y%m%d')}_{end.strftime('%Y%m%d')}"
    hash_key = hashlib.md5(key.encode('utf-8')).hexdigest()
    return CACHE_DIR / f"{ticker}_{hash_key[:8]}.pkl"


def fetch_paginated(
    url: str,
    params: Dict[str, Any],
    key: str,
    limit: int = 100,
    pause: float = 0.3
) -> pd.DataFrame:
    session = create_session()
    all_rows = []
    columns = None
    start_param = 0
    
    print(f"   Fetching {key} data...")
    
    while True:
        current_params = params.copy()
        current_params.update({
            "start": start_param,
            "limit": limit,
            "iss.meta": "off",
            "iss.only": key
        })
        
        try:
            response = session.get(url, params=current_params, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            container = data.get(key, {})
            rows = container.get("data", [])
            
            if columns is None:
                columns = container.get("columns", [])
            
            if not rows:
                print(f"      No more data at start={start_param}")
                break
            
            all_rows.extend(rows)
            print(f"      Got {len(rows)} rows (start={start_param})")
            
            start_param += len(rows)
            if len(rows) < limit:
                break
            
            time.sleep(pause)
            
        except requests.exceptions.RequestException as e:
            print(f"   ‚ö†Ô∏è  Network error: {e}, retrying...")
            time.sleep(pause * 2)
            if start_param > 10000:
                print("   ‚ùå Too many retries, aborting")
                break
            continue
    
    if not columns:
        raise ValueError(f"‚ùå No columns found in response for {key}")
    
    df = pd.DataFrame(all_rows, columns=columns)
    df.columns = [c.lower() for c in df.columns]
    
    return df


def load_daily_ticker(
    ticker: str,
    board: str,
    start: datetime,
    end: datetime,
    use_cache: bool = True,
) -> pd.DataFrame:  # –£–î–ê–õ–ï–ù –ù–ï–ò–°–ü–û–õ–¨–ó–£–ï–ú–´–ô –ü–ê–†–ê–ú–ï–¢–† fill_nan
    cache_path = get_cache_path(ticker, board, start, end)
    
    # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ –∫—ç—à–∞
    if use_cache and cache_path.exists():
        try:
            print(f"   üíæ Loading from cache: {cache_path.name}")
            with open(cache_path, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Cache load failed: {e}, reloading from API")
    
    # –ò–°–ü–†–ê–í–õ–ï–ù: –î–û–ë–ê–í–õ–ï–ù /history/ –î–õ–Ø –§–¨–Æ–ß–ï–†–°–û–í
    if board == "RFUD":
        url = f"{MOEX_BASE}/history/engines/futures/markets/forts/boards/{board}/securities/{ticker}.json"
    else:  # –ê–∫—Ü–∏–∏ –∏ –¥—Ä—É–≥–∏–µ
        url = f"{MOEX_BASE}/history/engines/stock/markets/shares/boards/{board}/securities/{ticker}.json"
    
    params = {
        "from": start.strftime("%Y-%m-%d"),
        "till": end.strftime("%Y-%m-%d"),
    }
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = fetch_paginated(url, params, key="history")
    
    if df.empty:
        print(f"   ‚ö†Ô∏è  No data received for {ticker}")
        return pd.DataFrame(columns=['date', 'open', 'high', 'low', 'close', 'volume', 'ticker'])
    
    # === –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ===
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–æ–Ω–æ–∫
    if "tradedate" in df.columns:
        df["date"] = pd.to_datetime(df["tradedate"]).dt.date
        df.drop(columns=["tradedate"], inplace=True)
    elif "trade_date" in df.columns:
        df["date"] = pd.to_datetime(df["trade_date"]).dt.date
        df.drop(columns=["trade_date"], inplace=True)
    else:
        # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –¥–∞—Ç–æ–π
        date_col = None
        for col in df.columns:
            if "date" in col.lower() and col != "date":
                date_col = col
                break
        if date_col:
            df["date"] = pd.to_datetime(df[date_col]).dt.date
            df.drop(columns=[date_col], inplace=True)
        else:
            raise ValueError(f"‚ùå No date column found for {ticker}")
    
    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ value -> volume
    if "value" in df.columns and "volume" not in df.columns:
        df.rename(columns={"value": "volume"}, inplace=True)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–∏–∫–µ—Ä–∞
    df["ticker"] = ticker
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∞—Ç–∞–º
    df = df[(df["date"] >= start.date()) & (df["date"] <= end.date())]
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    df = df.drop_duplicates(subset=["date", "ticker"])
    
    # –í—ã–±–æ—Ä –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    required_cols = ["date", "open", "high", "low", "close", "volume", "ticker"]
    available_cols = [c for c in required_cols if c in df.columns]
    
    # –ï—Å–ª–∏ —á–µ–≥–æ-—Ç–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç - –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
    missing_cols = [c for c in required_cols if c not in df.columns]
    if missing_cols:
        print(f"   ‚ö†Ô∏è  Missing columns for {ticker}: {missing_cols}, filling with 0")
        for col in missing_cols:
            df[col] = 0.0
    
    df = df[required_cols].copy()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
    if use_cache:
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(df, f)
            print(f"   üíæ Saved to cache: {cache_path.name}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Cache save failed: {e}")
    
    return df


def load_daily_multi(
    symbols: List[str],
    board: str,
    start: datetime,
    end: datetime,
    use_cache: bool = True,
) -> pd.DataFrame:
    parts = []
    global_min_date = end.date()
    global_max_date = start.date()
    
    print(f"\nüì• Loading data for {len(symbols)} symbols...")
    
    for ticker in symbols:
        try:
            print(f"\n   Loading {ticker}...")
            df = load_daily_ticker(ticker, board, start, end, use_cache)
            
            if df.empty:
                print(f"   ‚ö†Ô∏è  Empty data for {ticker}, skipping")
                continue
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
            global_min_date = min(global_min_date, df["date"].min())
            global_max_date = max(global_max_date, df["date"].max())
            
            parts.append(df)
            print(f"   ‚úÖ {ticker}: {df.shape[0]} rows")
            
        except Exception as e:
            print(f"   ‚ùå Error loading {ticker}: {e}")
            continue
    
    if not parts:
        print("‚ùå No data loaded for any symbol!")
        return pd.DataFrame()
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤
    print(f"\nüîÄ Merging {len(parts)} tickers...")
    df_all = pd.concat(parts, ignore_index=True)
    
    # === –°–û–ó–î–ê–ù–ò–ï –ü–û–õ–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê ===
    
    # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–∞–ª–µ–Ω–¥–∞—Ä—å —Ç–æ—Ä–≥–æ–≤—ã—Ö –¥–Ω–µ–π
    print("   Creating full date range...")
    full_dates = pd.date_range(
        start=global_min_date,
        end=global_max_date,
        freq='B'
    ).date
    
    # –°–æ–∑–¥–∞–µ–º DataFrame date + ticker
    print("   Creating base grid...")
    df_pivot = df_all.pivot(index="date", columns="ticker", values="close")
    df_pivot = df_pivot.reindex(full_dates)
    
    df_melted = df_pivot.rename_axis('date').reset_index().melt(
        id_vars=['date'],
        var_name='ticker',
        value_name='close'
    )
    
    other_cols = ['open', 'high', 'low', 'volume']
    for col in other_cols:
        if col in df_all.columns:
            pivot_col = df_all.pivot(index="date", columns="ticker", values=col)
            pivot_col = pivot_col.reindex(full_dates)
            
            melted_col = pivot_col.rename_axis('date').reset_index().melt(
                id_vars=['date'],
                var_name='ticker',
                value_name=col
            )
            
            df_melted = df_melted.merge(
                melted_col[['date', 'ticker', col]],
                on=['date', 'ticker'],
                how='left'
            )
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
    print("   Filling NaN values...")
    
    # –î–ª—è —Ü–µ–Ω: forward-fill
    price_cols = ['open', 'high', 'low', 'close']
    for col in price_cols:
        if col in df_melted.columns:
            df_melted[col] = df_melted.groupby('ticker')[col].fillna(method='ffill')
    
    # –î–ª—è –æ–±—ä–µ–º–∞: 0
    if 'volume' in df_melted.columns:
        df_melted['volume'] = df_melted['volume'].fillna(0)
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –≥–¥–µ –≤—Å–µ —Ü–µ–Ω—ã NaN
    df_melted = df_melted.dropna(subset=price_cols, how='all')
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    print("üìä Adding technical indicators...")
    df_melted = add_indicators_grouped(df_melted)
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
    df_melted = df_melted.sort_values(['date', 'ticker']).reset_index(drop=True)
    
    print(f"‚úÖ Final dataset: {df_melted.shape[0]} rows, {len(df_melted['ticker'].unique())} tickers")
    print(f"   Date range: {df_melted['date'].min()} ‚Üí {df_melted['date'].max()}")
    print(f"   Columns: {list(df_melted.columns)}")
    
    return df_melted


def add_indicators_grouped(df: pd.DataFrame) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
    """
    if df.empty:
        return df
    
    results = []
    
    for ticker, group in df.groupby("ticker"):
        print(f"      Processing indicators for {ticker}...")
        tmp = group.sort_values("date").reset_index(drop=True)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        if len(tmp) < 26:
            print(f"         ‚ö†Ô∏è  Not enough data for {ticker} ({len(tmp)} rows), skipping indicators")
            results.append(tmp)
            continue
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        try:
            tmp = feat.add_bollinger_bands(tmp, window=20, num_std=2.0)
            tmp = feat.calculate_macd(tmp, fast=12, slow=26, signal=9)
            tmp = feat.calculate_rsi(tmp, window=14)
            
            # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ
            tmp.rename(columns={
                "sma": "sma20",
                "bollinger_upper": "boll_upper20",
                "bollinger_lower": "boll_lower20",
                "rsi": "rsi14"
            }, inplace=True)
            
            results.append(tmp)
            
        except Exception as e:
            print(f"         ‚ùå Error calculating indicators for {ticker}: {e}")
            results.append(group)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    df_enriched = pd.concat(results, ignore_index=True)
    df_enriched = df_enriched.sort_values(['date', 'ticker']).reset_index(drop=True)
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –≤ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞—Ö (–¥–ª—è –ø–µ—Ä–≤—ã—Ö 20 –¥–Ω–µ–π)
    indicator_cols = ['sma20', 'boll_upper20', 'boll_lower20', 'macd', 'macd_signal', 'rsi14']
    for col in indicator_cols:
        if col in df_enriched.columns:
            df_enriched[col] = df_enriched.groupby('ticker')[col].fillna(method='ffill').fillna(0)
    
    return df_enriched


def demo_load_data():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print("=" * 80)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–ì–†–£–ó–ö–ò –î–ê–ù–ù–´–• –° MOEX")
    print("=" * 80)
    
    # –î–ª—è –∞–∫—Ü–∏–π
    print("\nüìà –ü—Ä–∏–º–µ—Ä 1: –ê–∫—Ü–∏–∏ MOEX (TQBR)")
    symbols = ["SBER", "GAZP", "LKOH"]
    board = "TQBR"
    start = datetime(2019, 1, 1)
    end = datetime(2022, 12, 31)
    
    print(f"–¢–∏–∫–µ—Ä—ã: {symbols}")
    print(f"–ü–µ—Ä–∏–æ–¥: {start.date()} - {end.date()}")
    print(f"–ë–æ—Ä–¥: {board}")
    
    df_stocks = load_daily_multi(symbols, board, start, end, use_cache=True)
    
    if not df_stocks.empty:
        print("\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(f"Shape: {df_stocks.shape}")
        print(f"–ö–æ–ª–æ–Ω–∫–∏: {list(df_stocks.columns)}")
        print(f"–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç: {df_stocks['date'].min()} - {df_stocks['date'].max()}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∏–∫–µ—Ä–æ–≤: {df_stocks['ticker'].nunique()}")
        print(f"\n–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:")
        print(df_stocks.head().to_string())
        print(f"\n–ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å—Ç—Ä–æ–∫:")
        print(df_stocks.tail().to_string())
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        output_file = Path(__file__).parent / "data" / "demo_stocks.csv"
        output_file.parent.mkdir(exist_ok=True)
        df_stocks.to_csv(output_file, index=False)
        print(f"\nüíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}")
    
    print("\n" + "=" * 80)
    print("‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")


if __name__ == "__main__":
    demo_load_data()